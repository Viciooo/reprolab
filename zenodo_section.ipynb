{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d337b1f3-3ce4-42a2-a7df-a8c611dec332",
   "metadata": {},
   "source": [
    "# ReproLab Demo\n",
    "\n",
    "Welcome to ReproLab! This extension helps you make your research more reproducible.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Create Experiments**: Automatically save immutable snapshots of your code under `git` tags to preserve the **exact code and outputs**\n",
    "- **Manage Dependencies**: Automatically gather and pin **exact package versions**, so that others can set up your environment with one command\n",
    "- **Cache Data**: Call external API/load manually dataset only once, caching function will handle the rest\n",
    "- **Archive Data**: Caching function can also preserve the compressed data in *AWS S3*, so you always know what data was used and reduce the API calls\n",
    "- **Publishing guide**: The reproducibility checklist & automated generation of reproducability package make publishing to platforms such as Zenodo very easy\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Use the sidebar to view ReproLab features\n",
    "2. Create virtual environment and pin your dependencies, go to reprolab section `Create reproducible environment` \n",
    "3. Create an experiment to save your current state, go to reprolab section `Create experiment`\n",
    "4. Archive your data for long-term storage, go to reprolab section `Demo` and play around with it.\n",
    "5. Publish your work when ready, remember to use reproducability checklist from the section `Reproducibility Checklist`\n",
    "\n",
    "## Example Usage of persistio decorator\n",
    "\n",
    "To cache and archive the datasets you use, both from local files and APIs we developed a simple decorator that put over your function that gets the datasets caches the file both locally and in the cloud so that the dataset you use is archived and the number of calls to external APIs is minimal and you don't need to keep the file around after you run it once.\n",
    "\n",
    "Here is an example using one of NASA open APIs. If you want to test it out yourself, you can copy the code, but you need to provide bucket name and access and secret key in the left-hand panel using the `AWS S3 Configuration` section.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# The two lines below is all that you need to add\n",
    "from reprolab.experiment import persistio\n",
    "@persistio()\n",
    "def get_exoplanets_data_from_nasa():\n",
    "    url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT TOP 10\n",
    "        pl_name AS planet_name,\n",
    "        hostname AS host_star,\n",
    "        pl_orbper AS orbital_period_days,\n",
    "        pl_rade AS planet_radius_earth,\n",
    "        disc_year AS discovery_year\n",
    "    FROM\n",
    "        ps\n",
    "    WHERE\n",
    "        default_flag = 1\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        df = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        print(df)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    return df\n",
    "\n",
    "exoplanets_data = get_exoplanets_data_from_nasa()\n",
    "```\n",
    "\n",
    "If you run this cell twice you will notice from the logs that the second time file was read from the compressed file in the cache. If you were to lose access to local cache (e.g. by pulling the repository using different device) `persistio` would fetch the data from the cloud archive.\n",
    "\n",
    "\n",
    "For more information, visit our [documentation](https://github.com/your-repo/reprolab). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fb5cc5-e35c-4995-94a0-5d1371750f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v1.5.0', 'v1.4.0', 'v1.3.0', 'v1.2.0', 'v1.1.0', 'v1.0.0']\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "def list_and_sort_git_tags(repo_path='.'):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['git', '-C', repo_path, 'tag'],\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        tags = result.stdout.strip().split('\\n')\n",
    "        tags = [tag for tag in tags if tag]\n",
    "\n",
    "        # Convert tags like v1.2.3 to 123 for sorting\n",
    "        def tag_to_sort_key(tag):\n",
    "            match = re.match(r'v(\\d+)\\.(\\d+)\\.(\\d+)', tag)\n",
    "            if match:\n",
    "                return int(''.join(match.groups()))\n",
    "            return -1  # Push malformed tags to the end\n",
    "\n",
    "        sorted_tags = sorted(tags, key=tag_to_sort_key, reverse=True)\n",
    "        return sorted_tags\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error listing tags: {e.stderr}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "sorted_tags = list_and_sort_git_tags()\n",
    "print(sorted_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf25ebc-1635-4afc-9031-5421aaa915b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip archive of tag 'v1.0.0' at v1.0.0.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def zip_git_tag(tag, repo_path='.', output_zip_path=None):\n",
    "    try:\n",
    "        if output_zip_path is None:\n",
    "            output_zip_path = f'{tag}.zip'\n",
    "        \n",
    "        # Create a temporary checkout of the tag in a separate directory\n",
    "        temp_dir = os.path.join(repo_path, f'__temp_checkout_{tag}')\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        os.makedirs(temp_dir)\n",
    "\n",
    "        # Use git archive to export the tag into a zip\n",
    "        subprocess.run(\n",
    "            ['git', '-C', repo_path, 'archive', '--format=zip', '--output', output_zip_path, tag],\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"Created zip archive of tag '{tag}' at {output_zip_path}\")\n",
    "        return True\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Git error: {e.stderr}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "zip_git_tag('v1.0.0', repo_path='.', output_zip_path='v1.0.0.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5825eb0c-7edc-48c9-bb27-ab041c79406a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metadata written to zenodo_section.ipynb_persistio_archive.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import datetime\n",
    "\n",
    "def persist_metadata_for_current_notebook(cell_hash, code_origin, bucket_name, notebook_name):\n",
    "    try:\n",
    "        yaml_filename = f\"{notebook_name}_persistio_archive.yaml\"\n",
    "        now_iso = datetime.datetime.now(datetime.UTC)\n",
    "\n",
    "        if os.path.exists(yaml_filename):\n",
    "            with open(yaml_filename, 'r') as f:\n",
    "                metadata = yaml.safe_load(f) or {}\n",
    "        else:\n",
    "            metadata = {}\n",
    "\n",
    "        metadata['jupyter_notebook'] = notebook_name\n",
    "        metadata['last_executed'] = now_iso\n",
    "\n",
    "        if 'creation_data' not in metadata:\n",
    "            metadata['creation_data'] = now_iso\n",
    "\n",
    "        if 'bucket_name' not in metadata:\n",
    "            metadata['bucket_name'] = bucket_name\n",
    "\n",
    "        if 'cells_instrumented' not in metadata:\n",
    "            metadata['cells_instrumented'] = []\n",
    "\n",
    "        existing = next((cell for cell in metadata['cells_instrumented'] if cell['hash'] == cell_hash), None)\n",
    "        if existing:\n",
    "            existing['code_origin'] = code_origin\n",
    "        else:\n",
    "            metadata['cells_instrumented'].append({\n",
    "                'hash': cell_hash,\n",
    "                'code_origin': code_origin\n",
    "            })\n",
    "\n",
    "        with open(yaml_filename, 'w') as f:\n",
    "            yaml.dump(metadata, f, sort_keys=False)\n",
    "\n",
    "        print(f\"✅ Metadata written to {yaml_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error persisting metadata: {e}\")\n",
    "\n",
    "persist_metadata_for_current_notebook('123', 'testing', 'bucket2', 'zenodo_section.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1125e03-1460-4561-8b7c-735c311ad757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zenodo_section.ipynb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_last_changed_notebook():\n",
    "    \"\"\"\n",
    "    Returns the name of the most recently modified Jupyter notebook (.ipynb) file \n",
    "    in the current directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all .ipynb files in the current directory\n",
    "        notebook_files = glob.glob('*.ipynb')\n",
    "        if not notebook_files:\n",
    "            raise RuntimeError(\"No .ipynb files found in the current directory\")\n",
    "        \n",
    "        # Get the most recently modified notebook\n",
    "        latest_notebook = max(notebook_files, key=os.path.getmtime)\n",
    "        return latest_notebook\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error finding last changed notebook: {str(e)}\")\n",
    "get_last_changed_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6744b1d6-a857-4725-b2f0-31008870f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[persistio] Function: get_exoplanets_data_from_nasa\n",
      "[persistio] Hash: ca840447667cb2059aa83ed68ec9e995\n",
      "[persistio] Attempting to load from local cache...\n",
      "[persistio] Successfully loaded from local cache!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# The two lines below is all that you need to add\n",
    "from reprolab.experiment import persistio\n",
    "@persistio()\n",
    "def get_exoplanets_data_from_nasa():\n",
    "    url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT TOP 10\n",
    "        pl_name AS planet_name,\n",
    "        hostname AS host_star,\n",
    "        pl_orbper AS orbital_period_days,\n",
    "        pl_rade AS planet_radius_earth,\n",
    "        disc_year AS discovery_year\n",
    "    FROM\n",
    "        ps\n",
    "    WHERE\n",
    "        default_flag = 1\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        df = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        print(df)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    return df\n",
    "\n",
    "exoplanets_data = get_exoplanets_data_from_nasa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_venv)",
   "language": "python",
   "name": "my_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
